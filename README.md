Llama 3.1 8B Bengali Dataset & Unsloth 2x Faster Finetuning
This repository contains a dataset and the associated finetuning script using the Llama 3.1 8B model with an optimization technique that speeds up the process by 2x. The finetuning is specifically targeted for the Bengali language, ensuring better accuracy and efficiency in NLP tasks.

Features
Model: Llama 3.1 (8B parameters)
Dataset: Bengali language dataset used for finetuning.(https://huggingface.co/datasets/iamshnoo/alpaca-cleaned-bengali)
Optimization: Unsloth method for 2x faster finetuning.
Objective: Achieve state-of-the-art performance on Bengali NLP tasks, such as text generation and classification.
Files
Llama_3_1_8b_Bengali_Dataset_+_Unsloth_2x_faster_finetuning.ipynb: Jupyter notebook with the full finetuning process and details of the dataset preparation.
Results
The finetuned model can be used for various Bengali language NLP tasks, providing faster and more accurate results due to the optimized finetuning process.

License
This project is licensed under the MIT License - see the LICENSE file for details.

Acknowledgments
The creators of the Llama 3.1 model.
Developers of the Unsloth optimization technique.
Contributors to the Bengali language dataset.
